{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c726e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(\"../../../../CNN_total/\")\n",
    "\n",
    "def get_parent_dir_name(path, level=1):\n",
    "\n",
    "    path_spliter = os.path.sep\n",
    "    abs_path = os.path.abspath(path)\n",
    "\n",
    "    return abs_path.split(path_spliter)[-(1 + level)]\n",
    "\n",
    "def get_frame_idx_list(total_frame, target_frame=32):\n",
    "    divide_num = target_frame - 1\n",
    "    target_frame_idx_list = [round(total_frame / divide_num * idx) for idx in range(target_frame)]\n",
    "    target_frame_idx_list[-1] = target_frame_idx_list[-1] - 1\n",
    "    return target_frame_idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4b6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_class_num: 11\n",
      "min_class_instance: 100\n",
      "max_class_instance: 163\n",
      "video_num: 1402\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "common_raw_data_folder = \"../data/0-2. Sampled/\"\n",
    "common_sample_folder = \"../data/1. Sampling png\"\n",
    "\n",
    "\n",
    "video_class_list = sorted(os.listdir(common_raw_data_folder))\n",
    "video_class_dict = {idx: class_str for idx, class_str in enumerate(video_class_list)}\n",
    "video_class_instance_num_list = [len(glob(f\"{common_raw_data_folder}/{video_class}/*\"))\n",
    "                                for video_class in video_class_list]\n",
    "\n",
    "video_path_list = glob(f\"{common_raw_data_folder}/*/*\")\n",
    "\n",
    "print(f\"video_class_num: {len(video_class_dict)}\")\n",
    "print(f\"min_class_instance: {np.min(video_class_instance_num_list)}\")\n",
    "print(f\"max_class_instance: {np.max(video_class_instance_num_list)}\")\n",
    "print(f\"video_num: {len(video_path_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577f476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1402it [09:51,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "sampling_folder = \"../data/1. Sampling png\"\n",
    "\n",
    "video_path_list = glob(f\"{common_raw_data_folder}/*/*\")\n",
    "target_frame = 16\n",
    "for idx, video_path in tqdm(enumerate(video_path_list)):\n",
    "    \n",
    "    class_str = get_parent_dir_name(video_path, level=1)\n",
    "    video_basename = get_parent_dir_name(video_path, level=0).replace(\".avi\", \"\")\n",
    "    video_array = skvideo.io.vread(video_path)\n",
    "    \n",
    "    video_array = video_array[:target_frame]\n",
    "    current_frame = len(video_array)\n",
    "    if current_frame == target_frame:\n",
    "        pass\n",
    "    else:\n",
    "        inter_ratio = target_frame / current_frame  \n",
    "        video_array = zoom(video_array, (inter_ratio, 1, 1, 1))\n",
    "    # video_shape: [frame // 5, 240, 320, 3]\n",
    "    total_frame = video_array.shape[0]\n",
    "    sample_folder = f\"{common_sample_folder}/{class_str}/{video_basename}\"\n",
    "    os.makedirs(sample_folder, exist_ok=True)\n",
    "        \n",
    "    for image_idx, image_array in enumerate(video_array):\n",
    "        image_path = f\"{sample_folder}/{image_idx:03}.png\"\n",
    "        Image.fromarray(image_array).save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d566574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "test_array = np.copy(video_array)\n",
    "result = zoom(test_array, (ratio, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca12280",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 128 / 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b1e329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 240, 320, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7bb11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 240, 320, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6f8f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1402it [17:10,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "sampling_folder = \"../data/1. Sampling png\"\n",
    "\n",
    "video_path_list = glob(f\"{common_raw_data_folder}/*/*\")\n",
    "target_frame = 32\n",
    "stride = 6\n",
    "for idx, video_path in tqdm(enumerate(video_path_list)):\n",
    "    \n",
    "    class_str = get_parent_dir_name(video_path, level=1)\n",
    "    video_basename = get_parent_dir_name(video_path, level=0).replace(\".avi\", \"\")\n",
    "    video_array = skvideo.io.vread(video_path)\n",
    "    \n",
    "    total_frame = video_array.shape[0] \n",
    "    frame_idx_list = get_frame_idx_list(total_frame, target_frame)\n",
    "    video_array = video_array[frame_idx_list]\n",
    "    \n",
    "    # video_shape: [frame // 5, 240, 320, 3]\n",
    "    total_frame = video_array.shape[0]\n",
    "    sample_folder = f\"{common_sample_folder}/{class_str}/{video_basename}\"\n",
    "    os.makedirs(sample_folder, exist_ok=True)\n",
    "        \n",
    "    for image_idx, image_array in enumerate(video_array):\n",
    "        image_path = f\"{sample_folder}/{image_idx:03}.png\"\n",
    "        Image.fromarray(image_array).save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeab7fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:06, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "sampling_folder = \"../data/1. Sampling png\"\n",
    "\n",
    "video_path_list = glob(f\"{common_raw_data_folder}/*/*\")\n",
    "target_fps = 5\n",
    "\n",
    "frame_per_sample = 8\n",
    "stride = 6\n",
    "for idx, video_path in tqdm(enumerate(video_path_list)):\n",
    "    \n",
    "    class_str = get_parent_dir_name(video_path, level=1)\n",
    "    video_basename = get_parent_dir_name(video_path, level=0).replace(\".avi\", \"\")\n",
    "    video_array = skvideo.io.vread(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(round(fps / 5))\n",
    "    video_array = video_array[0::frame_interval]    \n",
    "    # video_shape: [frame // 5, 240, 320, 3]\n",
    "    total_frame = video_array.shape[0]\n",
    "    sample_folder = f\"{common_sample_folder}/{class_str}/{video_basename}\"\n",
    "    \n",
    "    for frame_idx in range(0, total_frame - frame_per_sample, stride):\n",
    "        sample_array = video_array[frame_idx:frame_idx + frame_per_sample]\n",
    "        \n",
    "        sample_idx_folder = f\"{sample_folder}/{frame_idx:03}\"\n",
    "        os.makedirs(sample_idx_folder, exist_ok=True)\n",
    "        \n",
    "        for image_idx, image_array in enumerate(sample_array):\n",
    "            image_path = f\"{sample_idx_folder}/{image_idx:03}.png\"\n",
    "            Image.fromarray(image_array).save(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e9642",
   "metadata": {},
   "source": [
    "# Filter Unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a5bbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_sample_folder = \"../data/1. Sampling npy\"\n",
    "target_sample_list = os.listdir(\"../data/0-2. Sampled/\")\n",
    "\n",
    "sample_folder_list = glob(f\"{common_sample_folder}/*\")\n",
    "\n",
    "for sample_folder in sample_folder_list:\n",
    "    is_in = False\n",
    "    for target_sample in target_sample_list:\n",
    "        if target_sample in sample_folder:\n",
    "            is_in=True\n",
    "            break\n",
    "    if is_in:\n",
    "        pass\n",
    "    else:\n",
    "        shutil.rmtree(sample_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6dd9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 240, 320, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4027da86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 12,\n",
       " 18,\n",
       " 24,\n",
       " 30,\n",
       " 36,\n",
       " 42,\n",
       " 48,\n",
       " 54,\n",
       " 60,\n",
       " 66,\n",
       " 72,\n",
       " 78,\n",
       " 84,\n",
       " 90,\n",
       " 96,\n",
       " 102,\n",
       " 108,\n",
       " 114,\n",
       " 120,\n",
       " 126,\n",
       " 132,\n",
       " 138,\n",
       " 144,\n",
       " 150]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, total_frame - frame_per_sample, stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3107b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32b03c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 240, 320, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16bbc933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbeacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 240, 320, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a3371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from src.model.vision_transformer import swin_layers, transformer_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39416c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (28, 28, 1) # The image size of the MNIST\n",
    "patch_size = (2, 2) # Segment 28-by-28 frames into 2-by-2 sized patches, patch contents and positions are embedded\n",
    "n_labels = 10 # MNIST labels\n",
    "\n",
    "# Dropout parameters\n",
    "mlp_drop_rate = 0.01 # Droupout after each MLP layer\n",
    "attn_drop_rate = 0.01 # Dropout after Swin-Attention\n",
    "proj_drop_rate = 0.01 # Dropout at the end of each Swin-Attention block, i.e., after linear projections\n",
    "drop_path_rate = 0.01 # Drop-path within skip-connections\n",
    "\n",
    "# Self-attention parameters \n",
    "# (Fixed for all the blocks in this configuration, but can vary per block in larger architectures)\n",
    "num_heads = 8 # Number of attention heads\n",
    "embed_dim = 64 # Number of embedded dimensions\n",
    "num_mlp = 256 # Number of MLP nodes\n",
    "qkv_bias = True # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "qk_scale = None # None: Re-scale query based on embed dimensions per attention head # Float for user specified scaling factor\n",
    "\n",
    "# Shift-window parameters\n",
    "window_size = 2 # Size of attention window (height = width)\n",
    "shift_size = window_size // 2 # Size of shifting (shift_size < window_size)\n",
    "\n",
    "num_patch_x = input_size[0]//patch_size[0]\n",
    "num_patch_y = input_size[1]//patch_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a74aeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7a9187a92394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# The input section\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mIN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Extract patches from the input tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# The input section\n",
    "IN = Inputs(input_size)\n",
    "X = IN\n",
    "\n",
    "# Extract patches from the input tensor\n",
    "X = transformer_layers.patch_extract(patch_size)(X)\n",
    "\n",
    "# Embed patches to tokens\n",
    "X = transformer_layers.patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "\n",
    "# -------------------- Swin transformers -------------------- #\n",
    "# Stage 1: window-attention + Swin-attention + patch-merging\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        shift_size_temp = 0\n",
    "    else:\n",
    "        shift_size_temp = shift_size\n",
    "\n",
    "    X = swin_layers.SwinTransformerBlock(dim=embed_dim, num_patch=(num_patch_x, num_patch_y), num_heads=num_heads, \n",
    "                             window_size=window_size, shift_size=shift_size_temp, num_mlp=num_mlp, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                             mlp_drop=mlp_drop_rate, attn_drop=attn_drop_rate, proj_drop=proj_drop_rate, drop_path_prob=drop_path_rate, \n",
    "                             name='swin_block{}'.format(i))(X)\n",
    "# Patch-merging\n",
    "#    Pooling patch sequences. Half the number of patches (skip every two patches) and double the embedded dimensions\n",
    "X = transformer_layers.patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "# Convert embedded tokens (2D) to vectors (1D)\n",
    "X = GlobalAveragePooling1D()(X)\n",
    "\n",
    "# The output section\n",
    "OUT = Dense(n_labels, activation='softmax')(X)\n",
    "# Model configuration\n",
    "model = keras.models.Model(inputs=[IN,], outputs=[OUT,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b021e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
