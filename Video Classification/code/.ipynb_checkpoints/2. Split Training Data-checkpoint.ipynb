{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c726e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import sys\n",
    "import math\n",
    "sys.path.append(\"../../../../CNN_total/\")\n",
    "\n",
    "def get_parent_dir_name(path, level=1):\n",
    "\n",
    "    path_spliter = os.path.sep\n",
    "    abs_path = os.path.abspath(path)\n",
    "\n",
    "    return abs_path.split(path_spliter)[-(1 + level)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4b6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_class_num: 11\n",
      "min_class_instance: 100\n",
      "max_class_instance: 163\n",
      "video_num: 1402\n",
      "unique frame: [16]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "common_sample_folder = \"../data/1. Sampling png\"\n",
    "common_split_folder = \"../data/2. Split png\"\n",
    "\n",
    "video_class_list = sorted(os.listdir(common_sample_folder))\n",
    "video_class_dict = {idx: class_str for idx, class_str in enumerate(video_class_list)}\n",
    "video_class_instance_num_list = [len(glob(f\"{common_sample_folder}/{video_class}/*\"))\n",
    "                                for video_class in video_class_list]\n",
    "\n",
    "video_folder_list = glob(f\"{common_sample_folder}/*/*\")\n",
    "\n",
    "print(f\"video_class_num: {len(video_class_dict)}\")\n",
    "print(f\"min_class_instance: {np.min(video_class_instance_num_list)}\")\n",
    "print(f\"max_class_instance: {np.max(video_class_instance_num_list)}\")\n",
    "print(f\"video_num: {len(video_folder_list)}\")\n",
    "\n",
    "frame_num_list = []\n",
    "for video_folder in video_folder_list:\n",
    "    frame_num = len(os.listdir(video_folder))\n",
    "    frame_num_list.append(frame_num)\n",
    "    \n",
    "print(f\"unique frame: {np.unique(frame_num_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144c2adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:07, 15.92it/s]\n",
      "163it [00:10, 15.79it/s]\n",
      "134it [00:08, 16.25it/s]\n",
      "128it [00:07, 16.08it/s]\n",
      "123it [00:07, 16.25it/s]\n",
      "123it [00:07, 15.88it/s]\n",
      "127it [00:07, 16.17it/s]\n",
      "100it [00:06, 15.94it/s]\n",
      "160it [00:10, 15.95it/s]\n",
      "102it [00:06, 15.97it/s]\n",
      "130it [00:08, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "valid_ratio = 0.05\n",
    "test_ratio = 0.05\n",
    "\n",
    "for class_str in video_class_list:\n",
    "    video_folder_list = glob(f\"{common_sample_folder}/{class_str}/*\")\n",
    "    video_num = len(video_folder_list)\n",
    "    train_num = math.ceil(video_num * train_ratio)\n",
    "    valid_num = math.ceil(video_num * valid_ratio)\n",
    "    \n",
    "    for idx, (video_folder) in tqdm(enumerate(video_folder_list)):\n",
    "        video_id = get_parent_dir_name(video_folder, level=0)\n",
    "        \n",
    "        if idx <= train_num:\n",
    "            phase = \"train\"\n",
    "        elif idx <= train_num + valid_num:\n",
    "            phase = \"valid\"\n",
    "        else:\n",
    "            phase = \"test\"\n",
    "        split_video_folder = f\"{common_split_folder}/{phase}/{class_str}/{video_id}\"\n",
    "        shutil.copytree(video_folder, split_video_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc78027c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/2. Split png/train/BodyWeightSquats/v_BodyWeightSquats_g01_c01'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_video_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "529c8ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/1. Sampling png/BodyWeightSquats\\\\v_BodyWeightSquats_g01_c01\\\\000'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7e3617d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/2. Split png/train/BodyWeightSquats/v_BodyWeightSquats_g01_c01/000'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac05a48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/2. Split png/train/BodyWeightSquats/v_BodyWeightSquats_g01_c01/000'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e044c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "075f8b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v_BodyWeightSquats_g01_c01'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7a23522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/1. Sampling png/BodyWeightSquats\\\\v_BodyWeightSquats_g01_c01\\\\000'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2ea2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BodyWeightSquats 112\n",
      "BoxingPunchingBag 163\n",
      "BoxingSpeedBag 134\n",
      "HandstandPushups 128\n",
      "HighJump 123\n",
      "JumpingJack 123\n",
      "Lunges 127\n",
      "PullUps 100\n",
      "Punch 160\n",
      "PushUps 100\n",
      "WallPushups 130\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "valid_ratio = 0.05\n",
    "test_ratio = 0.05\n",
    "\n",
    "for class_str in class_list:\n",
    "    video_folder_list = glob(f\"{common_sample_folder}/{class_str}/*\")\n",
    "    print(class_str, len(video_folder_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6df904c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_folder_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e9642",
   "metadata": {},
   "source": [
    "# Filter Unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a5bbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_sample_folder = \"../data/1. Sampling npy\"\n",
    "target_sample_list = os.listdir(\"../data/0-2. Sampled/\")\n",
    "\n",
    "sample_folder_list = glob(f\"{common_sample_folder}/*\")\n",
    "\n",
    "for sample_folder in sample_folder_list:\n",
    "    is_in = False\n",
    "    for target_sample in target_sample_list:\n",
    "        if target_sample in sample_folder:\n",
    "            is_in=True\n",
    "            break\n",
    "    if is_in:\n",
    "        pass\n",
    "    else:\n",
    "        shutil.rmtree(sample_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6dd9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 240, 320, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4027da86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 12,\n",
       " 18,\n",
       " 24,\n",
       " 30,\n",
       " 36,\n",
       " 42,\n",
       " 48,\n",
       " 54,\n",
       " 60,\n",
       " 66,\n",
       " 72,\n",
       " 78,\n",
       " 84,\n",
       " 90,\n",
       " 96,\n",
       " 102,\n",
       " 108,\n",
       " 114,\n",
       " 120,\n",
       " 126,\n",
       " 132,\n",
       " 138,\n",
       " 144,\n",
       " 150]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, total_frame - frame_per_sample, stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3107b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32b03c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 240, 320, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16bbc933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbeacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 240, 320, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a3371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from src.model.vision_transformer import swin_layers, transformer_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39416c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (28, 28, 1) # The image size of the MNIST\n",
    "patch_size = (2, 2) # Segment 28-by-28 frames into 2-by-2 sized patches, patch contents and positions are embedded\n",
    "n_labels = 10 # MNIST labels\n",
    "\n",
    "# Dropout parameters\n",
    "mlp_drop_rate = 0.01 # Droupout after each MLP layer\n",
    "attn_drop_rate = 0.01 # Dropout after Swin-Attention\n",
    "proj_drop_rate = 0.01 # Dropout at the end of each Swin-Attention block, i.e., after linear projections\n",
    "drop_path_rate = 0.01 # Drop-path within skip-connections\n",
    "\n",
    "# Self-attention parameters \n",
    "# (Fixed for all the blocks in this configuration, but can vary per block in larger architectures)\n",
    "num_heads = 8 # Number of attention heads\n",
    "embed_dim = 64 # Number of embedded dimensions\n",
    "num_mlp = 256 # Number of MLP nodes\n",
    "qkv_bias = True # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "qk_scale = None # None: Re-scale query based on embed dimensions per attention head # Float for user specified scaling factor\n",
    "\n",
    "# Shift-window parameters\n",
    "window_size = 2 # Size of attention window (height = width)\n",
    "shift_size = window_size // 2 # Size of shifting (shift_size < window_size)\n",
    "\n",
    "num_patch_x = input_size[0]//patch_size[0]\n",
    "num_patch_y = input_size[1]//patch_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a74aeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7a9187a92394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# The input section\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mIN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Extract patches from the input tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# The input section\n",
    "IN = Inputs(input_size)\n",
    "X = IN\n",
    "\n",
    "# Extract patches from the input tensor\n",
    "X = transformer_layers.patch_extract(patch_size)(X)\n",
    "\n",
    "# Embed patches to tokens\n",
    "X = transformer_layers.patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "\n",
    "# -------------------- Swin transformers -------------------- #\n",
    "# Stage 1: window-attention + Swin-attention + patch-merging\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        shift_size_temp = 0\n",
    "    else:\n",
    "        shift_size_temp = shift_size\n",
    "\n",
    "    X = swin_layers.SwinTransformerBlock(dim=embed_dim, num_patch=(num_patch_x, num_patch_y), num_heads=num_heads, \n",
    "                             window_size=window_size, shift_size=shift_size_temp, num_mlp=num_mlp, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                             mlp_drop=mlp_drop_rate, attn_drop=attn_drop_rate, proj_drop=proj_drop_rate, drop_path_prob=drop_path_rate, \n",
    "                             name='swin_block{}'.format(i))(X)\n",
    "# Patch-merging\n",
    "#    Pooling patch sequences. Half the number of patches (skip every two patches) and double the embedded dimensions\n",
    "X = transformer_layers.patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "# Convert embedded tokens (2D) to vectors (1D)\n",
    "X = GlobalAveragePooling1D()(X)\n",
    "\n",
    "# The output section\n",
    "OUT = Dense(n_labels, activation='softmax')(X)\n",
    "# Model configuration\n",
    "model = keras.models.Model(inputs=[IN,], outputs=[OUT,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b021e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
