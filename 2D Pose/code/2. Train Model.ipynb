{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423552c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_number = \"0\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_number\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as keras_backend\n",
    "import numpy as np\n",
    "\n",
    "gpu_on = True\n",
    "\n",
    "if gpu_on :\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices(\"CPU\")\n",
    "\n",
    "print(gpu_devices)\n",
    "\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "sys.path.append(\"../../../../CNN_total/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_as_dict(json_path):\n",
    "    json_file = open(json_path, encoding=\"utf-8\")\n",
    "    json_str = json_file.read()\n",
    "    json_dict = json.loads(json_str)\n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "def is_in_condition(str_object):\n",
    "    is_in = False\n",
    "    for phase in [\"no_person\", \"single_person\"]:\n",
    "        if phase in str_object:\n",
    "            is_in = True\n",
    "            break\n",
    "    return is_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader.pose_estimation import PoseDataloader\n",
    "from glob import glob\n",
    "\n",
    "task = \"classification\"\n",
    "data_set_name = \"detect_lvi\"\n",
    "batch_size = 32\n",
    "num_workers = 12\n",
    "on_memory = False\n",
    "augmentation_proba = 0.625\n",
    "target_size = (256, 256)\n",
    "interpolation = \"bilinear\"\n",
    "# class_mode = \"binary\"\n",
    "class_mode = \"categorical\"\n",
    "dtype=\"float32\"\n",
    "\n",
    "train_image_path_list = glob(f\"../data/2. Split Data/train/*/*\")\n",
    "valid_image_path_list = glob(f\"../data/2. Split Data/valid/*/*\")\n",
    "test_image_path_list = glob(f\"../data/2. Split Data/test/*/*\")\n",
    "\n",
    "train_image_path_list = list(filter(is_in_condition, train_image_path_list))\n",
    "valid_image_path_list = list(filter(is_in_condition, valid_image_path_list))\n",
    "test_image_path_list = list(filter(is_in_condition, test_image_path_list))\n",
    "\n",
    "train_valid_annotation_dict = read_json_as_dict(\"../data/2. Split Data/single_person_keypoints_train2017.json\")\n",
    "test_annotation_dict = read_json_as_dict(\"../data/2. Split Data/single_person_keypoints_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_policy_dict = {\n",
    "    \"positional\": False,\n",
    "    \"noise\": True,\n",
    "    \"elastic\": False,\n",
    "    \"randomcrop\": False,\n",
    "    \"brightness_contrast\": False,\n",
    "    \"color\": False,\n",
    "    \"to_jpeg\": False\n",
    "}\n",
    "\n",
    "common_arg_dict = {\n",
    "    \"augmentation_policy_dict\": augmentation_policy_dict,\n",
    "    \"preprocess_input\": \"-1~1\",\n",
    "    \"target_size\": target_size,\n",
    "    \"interpolation\": interpolation,\n",
    "    \"dtype\": dtype\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ee54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = PoseDataloader(image_path_list=train_image_path_list,\n",
    "                                   total_annotation_dict=train_valid_annotation_dict,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_workers=num_workers,\n",
    "                                       on_memory=on_memory,\n",
    "                                       augmentation_proba=augmentation_proba,\n",
    "                                       shuffle=True,\n",
    "                                       **common_arg_dict\n",
    ")\n",
    "valid_data_loader = PoseDataloader(image_path_list=valid_image_path_list,\n",
    "                                   total_annotation_dict=train_valid_annotation_dict,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_workers=1,\n",
    "                                       on_memory=on_memory,\n",
    "                                       augmentation_proba=0,\n",
    "                                       shuffle=True,\n",
    "                                       **common_arg_dict\n",
    ")\n",
    "test_data_loader = PoseDataloader(image_path_list=test_image_path_list,\n",
    "                                   total_annotation_dict=test_annotation_dict,\n",
    "                                       batch_size=1,\n",
    "                                       num_workers=1,\n",
    "                                       on_memory=False,\n",
    "                                       augmentation_proba=0,\n",
    "                                       shuffle=False,\n",
    "                                       **common_arg_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66010d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape_list = []\n",
    "keypoints_shape_list = []\n",
    "label_shape_list = []\n",
    "for idx in range(100):\n",
    "    image, (keypoints, label) = train_data_loader[idx]\n",
    "    image_shape_list.append(image.shape)\n",
    "    keypoints_shape_list.append(keypoints.shape)\n",
    "    label_shape_list.append(label.shape)\n",
    "    \n",
    "print(set(image_shape_list))\n",
    "print(set(keypoints_shape_list))\n",
    "print(set(label_shape_list))\n",
    "print(np.unique(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.vision_transformer.pose_estimation import get_swin_pose_estimation_2d\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "last_channel_num = 11\n",
    "filter_num_begin = 128     # number of channels in the first downsampling block; it is also the number of embedded dimensions\n",
    "depth = 3                  # the depth of SwinUNET; depth=4 means three down/upsampling levels and a bottom level \n",
    "stack_num_per_depth = 2         # number of Swin Transformers per downsampling level\n",
    "patch_size = (4, 4)        # Extract 4-by-4 patches from the input image. Height and width of the patch must be equal.\n",
    "stride_mode = \"same\"\n",
    "num_heads = [4, 8, 8, 8]   # number of attention heads per down/upsampling level\n",
    "window_size = [4, 2, 2, 2] # the size of attention window per down/upsampling level\n",
    "num_mlp = 256              # number of MLP nodes within the Transformer\n",
    "act = \"gelu\"\n",
    "last_act = \"sigmoid\"\n",
    "shift_window = True          # Apply window shifting, i.e., Swin-MSA\n",
    "swin_v2 = True\n",
    "model = get_swin_pose_estimation_2d(input_shape, \n",
    "                                  filter_num_begin, depth, stack_num_per_depth,\n",
    "                                  patch_size, stride_mode, num_heads, window_size, num_mlp,\n",
    "                                  act=act, last_act=last_act, shift_window=shift_window, \n",
    "                                   swin_v2=swin_v2)\n",
    "print(f\"model param num: {model.count_params()}\")\n",
    "print(f\"model input shape: {model.input}\")\n",
    "print(f\"model output shape: {model.output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c15a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3000\n",
    "print(model(test_data_loader[idx][0])[0])\n",
    "print(test_data_loader[idx][1][0])\n",
    "print(model(test_data_loader[idx][0])[1])\n",
    "print(test_data_loader[idx][1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812834bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# YY/MM/dd\n",
    "today_str = today.strftime(\"%Y-%m-%d\")\n",
    "today_weight_path = f\"./result_daily/{task}/{data_set_name}/{today_str}/gpu_{gpu_number}/target_size_{target_size}/weights/\" \n",
    "today_image_path = f\"./result_daily/{task}/{data_set_name}/{today_str}/gpu_{gpu_number}/target_size_{target_size}/images/\"\n",
    "today_logs_path = f\"./result_daily/{task}/{data_set_name}/{today_str}/gpu_{gpu_number}/target_size_{target_size}/\"\n",
    "os.makedirs(today_weight_path, exist_ok=True)\n",
    "os.makedirs(today_logs_path, exist_ok=True)\n",
    "optimizer = Nadam(1e-4, clipnorm=1)\n",
    "\n",
    "save_c = ModelCheckpoint(\n",
    "    today_weight_path+\"/weights_{val_loss:.4f}_{loss:.4f}_{epoch:02d}.hdf5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min')\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 40:\n",
    "        new_lr = 2e-5\n",
    "    elif epoch <= 100:\n",
    "        new_lr = 2e-4\n",
    "    elif epoch <= 200:\n",
    "        new_lr = 2e-5\n",
    "    else:\n",
    "        new_lr = 2e-6\n",
    "    return new_lr\n",
    "scheduler_callback = LearningRateScheduler(scheduler, verbose=1)\n",
    "csv_logger = CSVLogger(f'{today_logs_path}/log.csv', append=False, separator=',')\n",
    "loss_function = MeanAbsoluteError()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "epochs = 300\n",
    "\n",
    "model.fit(\n",
    "    train_data_loader,\n",
    "    validation_data=valid_data_loader,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_data_loader),\n",
    "    validation_steps=len(valid_data_loader),    \n",
    "    callbacks=[scheduler_callback, save_c, csv_logger], \n",
    "    initial_epoch=start_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_path = f\"../data/val2017/{425226:012d}.jpg\"\n",
    "image_array = np.array(Image.open(image_path))\n",
    "\n",
    "for keypoint_idx in range(17):\n",
    "    x_idx, y_idx = keypoint_list[2 * keypoint_idx: 2 * (keypoint_idx + 1)]\n",
    "    label = label_list[keypoint_idx]\n",
    "    if label == 0:\n",
    "        continue\n",
    "    else:\n",
    "        if label == 1:\n",
    "            rgb_color = (255, 0, 0)\n",
    "        if label == 2:\n",
    "            rgb_color = (0, 0, 255)\n",
    "        image_array = cv2.circle(image_array, (x_idx, y_idx), radius=10, color=rgb_color, thickness=-1)\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec87586",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = f\"../data/val2017/{425226:012d}.jpg\"\n",
    "image_array = np.array(Image.open(image_path))\n",
    "keypoint_list, label_list = get_keypoint_array(425226, test_annotation_dict)\n",
    "image_array, keypoint_list = get_transform_image(image_array, keypoint_list, 1)\n",
    "\n",
    "for keypoint_idx in range(17):\n",
    "    x_idx, y_idx = keypoint_list[keypoint_idx]\n",
    "    label = label_list[keypoint_idx]\n",
    "    if label == 0:\n",
    "        continue\n",
    "    else:\n",
    "        if label == 1:\n",
    "            rgb_color = (255, 0, 0)\n",
    "        if label == 2:\n",
    "            rgb_color = (0, 0, 255)\n",
    "        image_array = cv2.circle(image_array, (x_idx, y_idx), radius=10, color=rgb_color, thickness=-1)\n",
    "        \n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a245da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
